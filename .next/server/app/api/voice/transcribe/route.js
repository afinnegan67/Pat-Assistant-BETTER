"use strict";(()=>{var e={};e.id=650,e.ids=[650],e.modules={399:e=>{e.exports=require("next/dist/compiled/next-server/app-page.runtime.prod.js")},517:e=>{e.exports=require("next/dist/compiled/next-server/app-route.runtime.prod.js")},2048:e=>{e.exports=require("fs")},9801:e=>{e.exports=require("os")},5315:e=>{e.exports=require("path")},7130:(e,t,o)=>{o.r(t),o.d(t,{originalPathname:()=>E,patchFetch:()=>C,requestAsyncStorage:()=>_,routeModule:()=>y,serverHooks:()=>T,staticGenerationAsyncStorage:()=>b});var r={};o.r(r),o.d(r,{POST:()=>v,PUT:()=>k});var n=o(9303),s=o(8716),a=o(670),i=o(7070),c=o(9313),l=o(2374),d=o(7997),p=o(1491),u=o(8618),g=o(5090);let m=`You are the transcript processor for Patrick's construction assistant. You analyze meeting transcripts and voice notes to extract:

1. TASKS: Action items, to-dos, things that need to be done
   - Include who mentioned it
   - Include any deadline mentioned
   - Include which project it relates to (if mentioned)

2. PROJECT KNOWLEDGE: Decisions made, information discussed, updates
   - Group by project
   - Keep chunks atomic (one fact/decision per chunk)
   - Include context for why the decision was made if available

3. NEW PROJECTS: Any new job sites or projects mentioned that don't exist

Be thorough but precise. Don't invent information not in the transcript.`;async function f(e,t){let o=(await (0,c.Yw)()).map(e=>e.name).join(", ");try{console.log("Transcript processing: calling AI with tool calling...");let{toolCalls:t}=await (0,l._4)({model:p.bt,tools:{extractFromTranscript:(0,d.w3)({description:"Extract tasks, knowledge, and new projects from the transcript",inputSchema:u.Bd})},toolChoice:{type:"tool",toolName:"extractFromTranscript"},system:m,prompt:`Existing projects: ${o||"None"}

Transcript:
${e}

Extract all tasks, knowledge, and new projects from this transcript. You MUST call the extractFromTranscript tool with your results.`});console.log("Transcript processing: got toolCalls response, count:",t?.length);let r=t[0];if(!r)throw console.error("Transcript processing: No tool call in response"),Error("No tool call result received");if(r.dynamic)throw console.error("Transcript processing: Got dynamic tool call instead of static"),Error("Dynamic tool call received");console.log("Transcript processing: extracting input from tool call");let n=r.input;return console.log("Transcript processing: result:",JSON.stringify(n,null,2)),n}catch(e){return console.error("Transcript processing error:",e),console.error("Error details:",e.message),{tasks:[],knowledge:[],new_projects:[]}}}async function w(e,t){let o=0,r=0,n=0,s=new Map;for(let t of e.new_projects){if(!t.name)continue;let e=await (0,c.ib)(t.name);if(e){s.set(t.name.toLowerCase(),e.id);continue}let o=await (0,c.$L)({name:t.name,client_name:t.client_name,project_type:t.project_type,status:"future"});s.set(t.name.toLowerCase(),o.id),n++}for(let t of e.tasks){if(!t.description)continue;let e=null;if(t.project_name){let o=await (0,c.ib)(t.project_name);e=o?.id||s.get(t.project_name.toLowerCase())||null}await (0,c.vr)({description:t.description,project_id:e,deadline:t.deadline,priority:t.priority||"medium"}),o++}for(let o of e.knowledge){if(!o.content)continue;let e=null;if(o.project_name){let t=await (0,c.ib)(o.project_name);e=t?.id||s.get(o.project_name.toLowerCase())||null}let n=null;try{n=await (0,g.C6)(o.content)}catch(e){console.error("Failed to generate embedding:",e)}await (0,c.tR)({project_id:e,content:o.content,embedding:n,source_type:"meeting",source_id:t}),r++}return{tasksCreated:o,knowledgeAdded:r,projectsCreated:n}}var h=o(2126);let j=process.env.ELEVENLABS_API_KEY;async function x(e){let t=new FormData;t.append("file",new Blob([new Uint8Array(e)]),"recording.webm"),t.append("model_id","scribe_v1");let o=await fetch("https://api.elevenlabs.io/v1/speech-to-text",{method:"POST",headers:{"xi-api-key":j},body:t});if(!o.ok){let e=await o.text();throw Error(`Transcription failed: ${e}`)}let r=await o.json(),n=r.text.split(/\s+/).length;return{text:r.text,duration:Math.round(n/150*60)}}async function v(e){console.log("=== VOICE TRANSCRIBE API START ===");try{let t=await e.formData(),o=t.get("audio"),r=t.get("duration");if(console.log("Audio file received:",o?.name,"size:",o?.size),!o)return console.log("No audio file provided"),i.NextResponse.json({error:"No audio file provided"},{status:400});let n=await o.arrayBuffer(),s=Buffer.from(n);console.log("Audio buffer created, length:",s.length),console.log("Starting ElevenLabs transcription...");let{text:a,duration:l}=await x(s);console.log("Transcription complete, text length:",a.length);let d=r?parseInt(r,10):l;console.log("Saving transcript to database...");let p=await (0,c.ME)({raw_content:a,duration_seconds:d,source:"webapp"});console.log("Transcript saved, ID:",p.id),console.log("Processing transcript with AI...");let u=await f(a,"webapp");console.log("Processing complete:",JSON.stringify(u,null,2));let g=function(e){let t=[];return(e.new_projects.length>0&&t.push(`New projects to create: ${e.new_projects.map(e=>e.name).join(", ")}`),e.tasks.length>0&&(t.push(`Tasks to create (${e.tasks.length}):`),e.tasks.slice(0,5).forEach(e=>{let o=e.project_name?` [${e.project_name}]`:"";t.push(`  - ${e.description}${o}`)}),e.tasks.length>5&&t.push(`  ... and ${e.tasks.length-5} more`)),e.knowledge.length>0&&t.push(`Knowledge chunks to store: ${e.knowledge.length}`),0===t.length)?"Nothing to extract from this transcript.":t.join("\n")}(u);if(d>1800)return await (0,h._b)(`Processed ${Math.round(d/60)} minute recording.

${g}

Reply "confirm" to save this data, or "cancel" to discard.`),i.NextResponse.json({success:!0,transcriptId:p.id,summary:g,needsConfirmation:!0,message:"Long recording - awaiting confirmation"});let m=await w(u,p.id);await (0,c.$3)(p.id,g);let j=`Processed recording: ${m.tasksCreated} tasks created, ${m.knowledgeAdded} knowledge chunks stored${m.projectsCreated>0?`, ${m.projectsCreated} new projects`:""}.`;return await (0,h._b)(j),i.NextResponse.json({success:!0,transcriptId:p.id,summary:g,needsConfirmation:!1,tasksCreated:m.tasksCreated,knowledgeAdded:m.knowledgeAdded,projectsCreated:m.projectsCreated})}catch(e){console.error("=== VOICE TRANSCRIBE ERROR ==="),console.error("Error type:",e.constructor.name),console.error("Error message:",e.message),console.error("Error stack:",e.stack);try{await (0,h._b)(`Voice transcription failed: ${e.message}`)}catch(e){console.error("Failed to send error notification:",e)}return i.NextResponse.json({error:"Transcription failed",details:e.message},{status:500})}}async function k(e){try{let{transcriptId:t,action:o}=await e.json();if(!t)return i.NextResponse.json({error:"No transcript ID provided"},{status:400});if("cancel"===o)return await (0,h._b)("Recording discarded."),i.NextResponse.json({success:!0,message:"Transcript discarded"});if("confirm"===o)return await (0,h._b)("Recording data saved."),i.NextResponse.json({success:!0,message:"Transcript committed"});return i.NextResponse.json({error:"Invalid action"},{status:400})}catch(e){return console.error("Confirmation error:",e),i.NextResponse.json({error:"Confirmation failed",details:e.message},{status:500})}}let y=new n.AppRouteRouteModule({definition:{kind:s.x.APP_ROUTE,page:"/api/voice/transcribe/route",pathname:"/api/voice/transcribe",filename:"route",bundlePath:"app/api/voice/transcribe/route"},resolvedPagePath:"C:\\Users\\aidan\\OneDrive\\Documents\\Pats Assitant Improved\\app\\api\\voice\\transcribe\\route.ts",nextConfigOutput:"",userland:r}),{requestAsyncStorage:_,staticGenerationAsyncStorage:b,serverHooks:T}=y,E="/api/voice/transcribe/route";function C(){return(0,a.patchFetch)({serverHooks:T,staticGenerationAsyncStorage:b})}}};var t=require("../../../../webpack-runtime.js");t.C(e);var o=e=>t(t.s=e),r=t.X(0,[276,460,562],()=>o(7130));module.exports=r})();